# data/15_fuse_suspensions.py

import pandas as pd
import numpy as np
from pymongo import MongoClient, UpdateOne, ASCENDING
from datetime import datetime
from tqdm import tqdm

# ---------------- Configuration ----------------
MONGO_HOST = "localhost"
MONGO_PORT = 27017
DB_NAME = "vnpy_stock"

COL_BAR = "bar_daily"  # æ—¥çº¿è¡Œæƒ…é›†åˆ
COL_EM_RAW = "suspension_daily_raw"  # ä¸œè´¢åŽŸå§‹åœç‰Œè¡¨
COL_CALENDAR = "trade_date_hist"  # äº¤æ˜“æ—¥åŽ†
COL_TARGET = "stock_status_history"  # æœ€ç»ˆç»“æžœè¡¨


# -----------------------------------------------

def get_db():
    return MongoClient(MONGO_HOST, MONGO_PORT)[DB_NAME]


def load_master_calendar(db):
    """
    åŠ è½½åŸºå‡†äº¤æ˜“æ—¥åŽ† (Numpy Array åŠ é€Ÿç‰ˆ)
    """
    print("ðŸ“… Loading Master Calendar...")
    # ä¼˜å…ˆå°è¯• trade_date_histï¼Œå…¶æ¬¡ trading_calendarï¼Œæœ€åŽ index_daily å…œåº•
    candidates = ["trade_date_hist", "trading_calendar", "index_daily"]

    dates = []
    for col_name in candidates:
        if col_name not in db.list_collection_names():
            continue

        field = "datetime" if col_name == "index_daily" else ("trade_date" if col_name == "trade_date_hist" else "date")
        query = {"symbol": "sh000001"} if col_name == "index_daily" else {}

        print(f"   Trying [{col_name}] with field='{field}'...")
        cursor = db[col_name].find(query, {field: 1, "_id": 0}).sort(field, ASCENDING)

        # å…¼å®¹æ€§è¯»å–ï¼šå¯èƒ½æ˜¯ str å¯èƒ½æ˜¯ datetime
        temp_dates = []
        for x in cursor:
            val = x.get(field)
            if val:
                temp_dates.append(pd.to_datetime(val))

        if temp_dates:
            dates = temp_dates
            break

    if not dates:
        raise RuntimeError("âŒ CRITICAL: No calendar found!")

    # è½¬ä¸º numpy array (datetime64[D]) ä»¥å®žçŽ°æžé€Ÿå·®é›†è¿ç®—
    cal_arr = np.array(dates, dtype='datetime64[D]')
    cal_arr = np.unique(cal_arr)  # åŽ»é‡
    cal_arr.sort()

    print(f"âœ… Master Calendar: {len(cal_arr)} days ({cal_arr[0]} to {cal_arr[-1]})")
    return cal_arr


def load_em_annotations(db):
    """
    åŠ è½½ä¸œè´¢åœç‰Œæ³¨è§£ï¼Œæž„å»ºå¿«é€ŸæŸ¥è¯¢å­—å…¸
    Key: (date_str, symbol) -> Value: reason
    """
    print("ðŸ“– Loading EM Suspension Annotations...")
    collection = db[COL_EM_RAW]
    # åªéœ€è¯»å–æ—¥æœŸã€ä»£ç ã€åŽŸå› 
    cursor = collection.find({}, {"date": 1, "symbol": 1, "reason": 1, "_id": 0})

    annotation_map = {}
    count = 0
    for doc in cursor:
        try:
            d_date = doc.get('date')
            symbol = doc.get('symbol')
            reason = doc.get('reason')

            if not d_date or not symbol:
                continue

            # å°†æ—¥æœŸè½¬ä¸ºå­—ç¬¦ä¸² Key (YYYY-MM-DD)
            # æ³¨æ„ï¼šMongoDB å­˜çš„å¯èƒ½æ˜¯ datetime æˆ– strï¼Œç»Ÿä¸€è½¬ str
            if isinstance(d_date, datetime):
                date_key = d_date.strftime("%Y-%m-%d")
            else:
                date_key = str(d_date).split(" ")[0]

            annotation_map[(date_key, symbol)] = reason
            count += 1
        except:
            continue

    print(f"âœ… Loaded {count} annotations into memory.")
    return annotation_map


def fuse_data(db, master_cal, em_map):
    """
    æ‰§è¡Œã€äº‹å®ž + æ³¨è§£ã€‘èžåˆ
    é€»è¾‘ï¼š
    1. äº‹å®ž(Fact): Volume > 0 çš„æ—¥å­ï¼Œç»å¯¹ä¸åœç‰Œã€‚
    2. ç¼ºå¤±(Gap):  äº¤æ˜“æ—¥åŽ†ä¸­æœ‰ï¼Œä½†äº‹å®žä¸­æ²¡æœ‰(ç¼ºè¡Œæˆ–Vol=0)çš„æ—¥å­ã€‚
    3. æ³¨è§£(Note): åœ¨ç¼ºå¤±æ—¥ï¼Œå¦‚æžœ EM è¡¨æœ‰è®°å½•ï¼Œç”¨ EM åŽŸå› ï¼›å¦åˆ™ç”¨å…œåº•åŽŸå› ã€‚
    """
    print("ðŸš€ Starting Data Fusion...")
    bar_col = db[COL_BAR]
    target_col = db[COL_TARGET]

    # èŽ·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
    stocks = bar_col.distinct("symbol")
    stocks.sort()

    ops = []

    for symbol in tqdm(stocks):
        try:
            # 1. èŽ·å–â€œæœ‰æ•ˆäº¤æ˜“æ—¥â€ (Volume > 0)
            # å¿…é¡»è¯»å– volume å­—æ®µ
            cursor = bar_col.find(
                {"symbol": symbol},
                {"date": 1, "datetime": 1, "volume": 1, "_id": 0}
            )

            active_dates_list = []
            for doc in cursor:
                # --- å…¼å®¹æ€§æ—¥æœŸè¯»å– (ä½ è¦æ±‚çš„) ---
                d = doc.get("datetime") or doc.get("date")
                if not d:
                    continue

                # --- æ ¸å¿ƒåˆ¤å®šé€»è¾‘ ---
                # å¦‚æžœ Volume > 0ï¼Œè§†ä¸ºåœ¨åœºäº¤æ˜“
                # å¦‚æžœ Volume = 0ï¼Œè§†ä¸ºç¦»åœº(åœç‰Œå€™é€‰)ï¼Œä¸åŠ å…¥ active_dates
                vol = doc.get("volume", 0)
                if vol > 0:
                    active_dates_list.append(pd.to_datetime(d))

            if not active_dates_list:
                continue

            # è½¬ä¸º numpy array
            active_dates = np.array(active_dates_list, dtype='datetime64[D]')
            active_dates.sort()

            # 2. ç¡®å®šç”Ÿå‘½å‘¨æœŸ (ä¸Šå¸‚æ—¥ ~ æœ€æ–°æœ‰äº¤æ˜“æ—¥)
            min_date = active_dates[0]
            max_date = active_dates[-1]

            # 3. æˆªå–ç†è®ºæ—¥åŽ† (Expected)
            start_idx = np.searchsorted(master_cal, min_date)
            end_idx = np.searchsorted(master_cal, max_date, side='right')
            expected_slice = master_cal[start_idx:end_idx]

            # 4. è®¡ç®—ç¼ºå¤±æ—¥ (Gaps = Expected - Active)
            # è¿™é‡Œé¢åŒ…å«äº†ï¼šçœŸæ­£ç¼ºæ•°æ®çš„æ—¥å­ + Volume=0 çš„æ—¥å­
            susp_dates = np.setdiff1d(expected_slice, active_dates)

            if len(susp_dates) == 0:
                continue

            # 5. åŒºé—´åˆå¹¶ä¸ŽåŽŸå› åŒ¹é…
            intervals = []

            # è¾…åŠ©å‡½æ•°ï¼šæäº¤ä¸€ä¸ªè¿žç»­åœç‰ŒåŒºé—´
            def commit_chunk(chunk_dates):
                if len(chunk_dates) == 0:
                    return

                start_dt = pd.to_datetime(chunk_dates[0])
                end_dt = pd.to_datetime(chunk_dates[-1])

                # å¯»æ‰¾åŽŸå› ï¼šåªè¦åŒºé—´å†…ä»»ä½•ä¸€å¤©åœ¨ EM Map é‡Œæœ‰è®°å½•ï¼Œå°±é‡‡ç”¨è¯¥è®°å½•
                # ä¼˜å…ˆåŒ¹é… start_date (é€šå¸¸å…¬å‘Šå‘åœ¨åœç‰Œé¦–æ—¥)
                best_reason = "Missing Data / Zero Vol"
                source_tag = "inference"

                for date_np in chunk_dates:
                    # è½¬ä¸º YYYY-MM-DD ç”¨äºŽæŸ¥å­—å…¸
                    day_str = str(date_np)
                    key = (day_str, symbol)

                    if key in em_map:
                        best_reason = em_map[key]
                        source_tag = "eastmoney_confirmed"
                        # åªè¦æ‰¾åˆ°ä¸€ä¸ªåŽŸå› ï¼Œå°±è®¤ä¸ºæ•´ä¸ªåŒºé—´æ˜¯å› ä¸ºè¿™ä»¶äº‹ï¼Œç›´æŽ¥è·³å‡º
                        break

                intervals.append({
                    "start": start_dt,
                    "end": end_dt,
                    "reason": best_reason,
                    "source": source_tag
                })

            # ä½¿ç”¨ Numpy æŠ€å·§å¿«é€Ÿåˆ†ç»„è¿žç»­æ—¥æœŸ
            # é€»è¾‘ï¼šä¸¤ä¸ªæ—¥æœŸå¦‚æžœåœ¨æ—¥åŽ†ä¸Šè¿žç»­ï¼Œå®ƒä»¬åœ¨ master_cal çš„ index å·®å€¼åº”ä¸º 1
            # æˆ‘ä»¬éœ€è¦æ‰¾åˆ° susp_dates åœ¨ master_cal ä¸­çš„åŽŸå§‹ç´¢å¼•
            mask = np.isin(master_cal, susp_dates)
            all_indices = np.where(mask)[0]

            if len(all_indices) > 0:
                # Grouping: index - arange
                groups = all_indices - np.arange(len(all_indices))
                unique_groups = np.unique(groups)

                for g in unique_groups:
                    group_indices = all_indices[groups == g]
                    group_dates = master_cal[group_indices]
                    commit_chunk(group_dates)

            # 6. æž„é€ å†™å…¥è¯·æ±‚
            if intervals:
                ops.append(
                    UpdateOne(
                        {"symbol": symbol},
                        {"$set": {
                            "suspensions": intervals,  # æœ€ç»ˆä½¿ç”¨çš„å­—æ®µ
                            "suspension_updated_at": datetime.now()
                        }},
                        upsert=True
                    )
                )

        except Exception as e:
            # print(f"Error {symbol}: {e}")
            continue

        if len(ops) >= 1000:
            target_col.bulk_write(ops)
            ops = []

    if ops:
        target_col.bulk_write(ops)

    print("\nâœ… Fusion Completed! Your data is now Production-Ready.")


if __name__ == "__main__":
    db = get_db()

    # 1. å‡†å¤‡æ—¥åŽ†
    master_calendar = load_master_calendar(db)

    # 2. å‡†å¤‡æ³¨è§£ (ä¹‹å‰ä¸‹è½½çš„ä¸œè´¢æ•°æ®)
    em_annotation_map = load_em_annotations(db)

    # 3. èžåˆ
    if len(master_calendar) > 0:
        fuse_data(db, master_calendar, em_annotation_map)