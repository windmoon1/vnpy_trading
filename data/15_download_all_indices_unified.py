"""
Script 15 (V11.0): Download All Indices (Robust & Normalized)
-------------------------------------------------------------
ç›®æ ‡: [æ¸…åº“é‡ç½®ç‰ˆ] ç»Ÿä¸€ä¸‹è½½ [å®½åŸº]ã€[è¡Œä¸š]ã€[æ¦‚å¿µ] ä¸‰ç±»æŒ‡æ•°ã€‚
æ¶æ„: Code-First + Unified Schema + Normalized Symbols

æ”¹è¿›:
  1. [Normalize] è¡Œä¸š/æ¦‚å¿µä»£ç å¼ºåˆ¶æ·»åŠ  'BK' å‰ç¼€ (å¦‚ BK0475)ï¼Œç¡®ä¿ DB ä¸»é”®ç»Ÿä¸€ã€‚
  2. [Index] è‡ªåŠ¨åˆ›å»º MongoDB ç´¢å¼•ï¼ŒåŠ é€Ÿæ–­ç‚¹æŸ¥è¯¢ã€‚
  3. [Feedback] è¿›åº¦æ¡æ˜¾å¼å±•ç¤º Skip æ•°é‡ã€‚

Schema:
  symbol, date, category, name,
  open, high, low, close, volume,
  turnover, turnover_rate, amplitude, change_pct
"""

import akshare as ak
import pandas as pd
import time
import random
import sys
import os
import datetime
from tqdm import tqdm
from pymongo import MongoClient, UpdateOne, ASCENDING

# å¼•å…¥å·¥å…·
sys.path.append(os.path.abspath(os.path.dirname(__file__)))
from utils.network_guard import NetworkGuard
from utils.fix_akshare import apply_patches

# --- é…ç½® ---
MONGO_HOST = "localhost"
MONGO_PORT = 27017
DB_NAME = "vnpy_stock"
MAX_RETRIES = 5

START_DATE = "19900101"
END_DATE = datetime.datetime.now().strftime("%Y%m%d")

BENCHMARKS = [
    ("sh000001", "ä¸Šè¯æŒ‡æ•°"), ("sz399001", "æ·±è¯æˆæŒ‡"), ("sz399006", "åˆ›ä¸šæ¿æŒ‡"),
    ("sh000300", "æ²ªæ·±300"), ("sh000905", "ä¸­è¯500"), ("sh000852", "ä¸­è¯1000"),
    ("sh000688", "ç§‘åˆ›50"), ("sh000016", "ä¸Šè¯50"),
    ("sh000985", "ä¸­è¯å…¨æŒ‡"), ("sz899050", "åŒ—è¯50"),
]

client = MongoClient(MONGO_HOST, MONGO_PORT)
db = client[DB_NAME]

def ensure_indexes():
    """åˆ›å»ºç´¢å¼•åŠ é€ŸæŸ¥è¯¢"""
    print("ğŸ”¨ æ­£åœ¨ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•...")
    db["index_daily"].create_index([("symbol", ASCENDING), ("datetime", -1)])
    db["index_daily"].create_index([("category", ASCENDING)])

def normalize_bk_code(code: str) -> str:
    """æ ‡å‡†åŒ–æ¿å—ä»£ç : 0475 -> BK0475"""
    code = str(code).strip()
    if not code.startswith("BK"):
        return f"BK{code}"
    return code

def get_db_latest_date(symbol):
    """æŸ¥è¯¢æ•°æ®åº“æœ€æ–°æ—¥æœŸ"""
    doc = db["index_daily"].find_one(
        {"symbol": symbol},
        sort=[("datetime", -1)],
        projection={"datetime": 1}
    )
    return doc["datetime"] if doc else None

def retry_action(func, *args, **kwargs):
    for attempt in range(MAX_RETRIES):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            err_msg = str(e)
            if "Length mismatch" in err_msg or "char 0" in err_msg: return pd.DataFrame()
            if "ProxyError" in err_msg or "ConnectionPool" in err_msg:
                time.sleep(random.uniform(3, 8))
                NetworkGuard.rotate_identity()
            if attempt == MAX_RETRIES - 1: pass
            time.sleep(random.uniform(1, 3))
    return None

def standardize_columns(df):
    if df is None or df.empty: return None
    rename_map = {
        "date": "date", "amount": "turnover",
        "æ—¥æœŸ": "date", "å¼€ç›˜": "open", "æœ€é«˜": "high", "æœ€ä½": "low", "æ”¶ç›˜": "close",
        "æˆäº¤é‡": "volume", "æˆäº¤é¢": "turnover", "æ¢æ‰‹ç‡": "turnover_rate",
        "æ¶¨è·Œå¹…": "change_pct", "æŒ¯å¹…": "amplitude"
    }
    cols_to_rename = {k: v for k, v in rename_map.items() if k in df.columns}
    df.rename(columns=cols_to_rename, inplace=True)

    for col in ["turnover_rate", "change_pct"]:
        if col not in df.columns: df[col] = 0.0

    if "amplitude" not in df.columns:
        df = df.sort_values("date").reset_index(drop=True)
        pre_close = df["close"].shift(1)
        amplitude = (df["high"] - df["low"]) / pre_close * 100
        df["amplitude"] = amplitude.fillna(0.0)

    required = ["date", "open", "high", "low", "close", "volume", "turnover", "turnover_rate", "change_pct", "amplitude"]
    for col in required:
        if col not in df.columns: return None
    return df[required]

def process_one_symbol(store_symbol, query_symbol, name, category, fetch_func, **kwargs):
    """
    :param store_symbol: å­˜åº“ä»£ç  (BK0475)
    :param query_symbol: æŸ¥è¯¢ä»£ç  (BK0475 / 0475 / sh000300)
    """

    # 1. æ–­ç‚¹è·³è¿‡
    last_date = get_db_latest_date(store_symbol)
    yesterday = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime("%Y-%m-%d")

    if last_date and last_date >= yesterday:
        return "SKIPPED"

    # 2. ä¸‹è½½
    df = retry_action(fetch_func, symbol=query_symbol, **kwargs)

    # 3. æ ‡å‡†åŒ–
    df = standardize_columns(df)
    if df is None or df.empty: return "EMPTY"

    # 4. å…¥åº“
    ops = []
    for _, row in df.iterrows():
        try:
            date_str = str(row["date"])[:10]
            doc = {
                "symbol": store_symbol,
                "exchange": "INDEX",
                "datetime": date_str,
                "interval": "d",
                "category": category,
                "name": name,
                "open": float(row["open"]),
                "high": float(row["high"]),
                "low": float(row["low"]),
                "close": float(row["close"]),
                "volume": float(row["volume"]),
                "turnover": float(row["turnover"]),
                "turnover_rate": float(row["turnover_rate"]),
                "change_pct": float(row["change_pct"]),
                "amplitude": float(row["amplitude"])
            }
            ops.append(UpdateOne({"symbol": store_symbol, "datetime": date_str}, {"$set": doc}, upsert=True))
        except: continue

    if ops:
        db["index_daily"].bulk_write(ops, ordered=False)
        return "UPDATED"
    return "EMPTY"

def run_unified_job():
    print("ğŸš€ å¯åŠ¨ [å…¨æŒ‡æ•°] ç»Ÿä¸€ä¸‹è½½ä»»åŠ¡ (V11.0 Robust)...")
    ensure_indexes()

    apply_patches()
    NetworkGuard.install()

    # --- 1. å®½åŸº ---
    print(f"\nğŸ“Š [1/3] å®½åŸºæŒ‡æ•° ({len(BENCHMARKS)} ä¸ª)...")
    for symbol, name in tqdm(BENCHMARKS, desc="Benchmark"):
        process_one_symbol(
            store_symbol=symbol, query_symbol=symbol, name=name, category="BENCHMARK",
            fetch_func=ak.stock_zh_index_daily_em
        )

    # --- 2. è¡Œä¸š ---
    print("\nğŸ“Š [2/3] è¡Œä¸šæ¿å— (Normalized BK)...")
    try:
        em_ind_df = ak.stock_board_industry_name_em()
        # æ ‡å‡†åŒ–: å­˜åº“ç”¨ BKxxxx, æŸ¥è¯¢ç”¨ BKxxxx (AKShare æ”¯æŒ)
        em_ind_list = [{"code": normalize_bk_code(r["æ¿å—ä»£ç "]), "name": r["æ¿å—åç§°"]} for _, r in em_ind_df.iterrows()]

        pbar = tqdm(em_ind_list, desc="Industry")
        stats = {"skip": 0, "upd": 0}

        for item in pbar:
            status = process_one_symbol(
                store_symbol=item['code'], query_symbol=item['code'],
                name=item['name'], category="INDUSTRY",
                fetch_func=ak.stock_board_industry_hist_em,
                start_date=START_DATE, end_date=END_DATE
            )
            if status == "UPDATED":
                stats["upd"] += 1
                time.sleep(random.uniform(0.5, 1.5))
            elif status == "SKIPPED":
                stats["skip"] += 1

            pbar.set_postfix(skip=stats['skip'], upd=stats['upd'])

    except Exception as e: print(f"âŒ è¡Œä¸šå¤±è´¥: {e}")

    # --- 3. æ¦‚å¿µ ---
    print("\nğŸ“Š [3/3] æ¦‚å¿µæ¿å— (Normalized BK)...")
    try:
        try:
            em_con_df = retry_action(ak.stock_board_concept_name_em)
            if em_con_df is None: raise Exception("API Error")
            em_con_list = [{"code": normalize_bk_code(r["æ¿å—ä»£ç "]), "name": r["æ¿å—åç§°"]} for _, r in em_con_df.iterrows()]
        except:
            print("   âš ï¸ åˆ‡æ¢æœ¬åœ°ç¼“å­˜...")
            cursor = db["index_info"].find({"category": "CONCEPT"}, {"name": 1, "symbol": 1})
            em_con_list = [{"code": normalize_bk_code(d["symbol"]), "name": d["name"]} for d in cursor]

        pbar = tqdm(em_con_list, desc="Concept")
        stats = {"skip": 0, "upd": 0}

        for item in pbar:
            status = process_one_symbol(
                store_symbol=item['code'], query_symbol=item['code'],
                name=item['name'], category="CONCEPT",
                fetch_func=ak.stock_board_concept_hist_em,
                start_date=START_DATE, end_date=END_DATE, period="daily"
            )
            if status == "UPDATED":
                stats["upd"] += 1
                time.sleep(random.uniform(1.0, 3.0))
            elif status == "SKIPPED":
                stats["skip"] += 1

            pbar.set_postfix(skip=stats['skip'], upd=stats['upd'])

    except Exception as e: print(f"âŒ æ¦‚å¿µå¤±è´¥: {e}")

    print("\nâœ¨ ä»»åŠ¡å®Œæˆã€‚")

if __name__ == "__main__":
    try:
        run_unified_job()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·åœæ­¢ã€‚")