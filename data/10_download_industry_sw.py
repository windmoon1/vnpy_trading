"""
Script 10: Rebuild Industry History with Full Hierarchy
-------------------------------------------------------
ç›®æ ‡: é‡æ„è¡Œä¸šå†å²è¡¨ (industry_history)
æµç¨‹:
1. [æ¸…ç†] æ¸…ç©ºç°æœ‰ industry_history è¡¨ (ä»é›¶å¼€å§‹)ã€‚
2. [æ˜ å°„] è¯»å–æœ¬åœ° sw_2021.csv (Sheet1), æ„å»ºå…¨å±‚çº§æ˜ å°„å­—å…¸ (Code -> L1/L2/L3)ã€‚
3. [ä¸‹è½½] åœ¨çº¿æ‹‰å–ç”³ä¸‡ä¸ªè‚¡å†å²æ•°æ® (ak.stock_industry_clf_hist_sw)ã€‚
4. [å…¥åº“] å°†å†å²æ•°æ®çš„ Code ç¿»è¯‘ä¸ºå…¨å±‚çº§ç»“æ„å¹¶å­˜å‚¨ã€‚

æ³¨æ„:
å¦‚æœçº¿ä¸Šæ•°æ®åŒ…å«æ—§ç‰ˆä»£ç (å¦‚4xxxx)è€ŒSheet1åªæœ‰æ–°ç‰ˆä»£ç (11xxxx),
æœªåŒ¹é…çš„è®°å½•å°†åªå­˜å‚¨åŸå§‹ä»£ç ï¼Œæ ‡è®° is_mapped=Falseã€‚
"""

import akshare as ak
import pandas as pd
import os
from datetime import datetime
from tqdm import tqdm
from pymongo import MongoClient, UpdateOne

# ==========================================
# é…ç½®
# ==========================================
MONGO_HOST = "localhost"
MONGO_PORT = 27017
DB_NAME = "vnpy_stock"
COLLECTION_NAME = "industry_history"
MAPPING_FILE = "data/è¡Œä¸šåˆ†ç±».csv"  # è¯·ç¡®ä¿ä½ å·²å°† Sheet1.csv é‡å‘½åä¸ºæ­¤æ–‡ä»¶å

def get_db():
    return MongoClient(MONGO_HOST, MONGO_PORT)[DB_NAME]

def load_full_hierarchy_map(file_path):
    """
    ä» CSV æ„å»ºå…¨ç»´åº¦æ˜ å°„å­—å…¸
    Dict Structure:
    {
        '110101': {'l1_c': '110000', 'l1_n': 'å†œæ—ç‰§æ¸”', 'l2_c': ..., 'l3_n': 'ç§å­'},
        '801010': {'l1_c': '...', ...} (å…¼å®¹æŒ‡æ•°ä»£ç )
    }
    """
    print(f"ğŸ“š æ­£åœ¨åŠ è½½æ˜ å°„æ–‡ä»¶: {file_path}")
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return {}

    try:
        # å¼ºåˆ¶è¯»å–ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…ä»£ç å‰å¯¼0ä¸¢å¤±
        df = pd.read_csv(file_path, dtype=str)

        # æ¸…ç†åˆ—åç©ºæ ¼
        df.columns = [c.strip() for c in df.columns]

        mapping = {}

        for _, row in df.iterrows():
            # æå–å„çº§ä¿¡æ¯ (å¤„ç†å¯èƒ½çš„ç©ºå€¼)
            l1_c = str(row.get('industry_level1_code', '')).strip()
            l1_n = str(row.get('industry_level1_name', '')).strip()
            l2_c = str(row.get('industry_level2_code', '')).strip()
            l2_n = str(row.get('industry_level2_name', '')).strip()
            l3_c = str(row.get('industry_level3_code', '')).strip()
            l3_n = str(row.get('industry_level3_name', '')).strip()

            # æ„é€ å®Œæ•´æ•°æ®åŒ…
            full_info = {
                "level1_code": l1_c, "level1_name": l1_n,
                "level2_code": l2_c, "level2_name": l2_n,
                "level3_code": l3_c, "level3_name": l3_n
            }

            # ç­–ç•¥: å°†æ‰€æœ‰å±‚çº§çš„ä»£ç éƒ½ä½œä¸º Key æŒ‡å‘è¿™ä¸ª Info
            # è¿™æ ·æ— è®º API è¿”å›çš„æ˜¯ä¸€çº§è¿˜æ˜¯ä¸‰çº§ä»£ç ï¼Œéƒ½èƒ½æŸ¥åˆ°å®¶æ—ä¿¡æ¯

            if l3_c and l3_c.lower() != 'nan': mapping[l3_c] = full_info
            if l2_c and l2_c.lower() != 'nan':
                # å¦‚æœ L2 å·²ç»ä½œä¸º Key å­˜åœ¨ (å¯èƒ½æ¥è‡ªå¦ä¸€è¡Œ)ï¼Œä¸è¦è¦†ç›–ï¼Œå› ä¸º L2 å¯¹åº”å¤šä¸ª L3
                # ä½†å¯¹äº"æŸ¥è¯¢ L2 å±äºå“ªä¸ª L1"ï¼Œä»»æ„ä¸€è¡Œéƒ½æ˜¯ä¸€æ ·çš„ã€‚
                # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬åªå­˜ç¬¬ä¸€æ¬¡å‡ºç°çš„æ˜ å°„ (L2 -> L1 å…³ç³»æ˜¯å›ºå®šçš„)
                if l2_c not in mapping:
                    mapping[l2_c] = full_info
            if l1_c and l1_c.lower() != 'nan':
                if l1_c not in mapping:
                    mapping[l1_c] = full_info

        print(f"âœ… æ˜ å°„å­—å…¸æ„å»ºå®Œæˆï¼Œç´¢å¼•æ•°: {len(mapping)}")
        return mapping

    except Exception as e:
        print(f"âŒ è¯»å– CSV å¤±è´¥: {e}")
        return {}

def run():
    print("ğŸš€ å¯åŠ¨ [è¡Œä¸šå…¨é‡é‡æ„è„šæœ¬]...")
    db = get_db()
    col = db[COLLECTION_NAME]

    # 1. æ¸…ç©ºæ—§æ•°æ® (æ…é‡æ“ä½œ)
    print(f"ğŸ—‘ï¸  æ­£åœ¨æ¸…ç©ºè¡¨ [{COLLECTION_NAME}]...")
    col.delete_many({})
    print("   å·²æ¸…ç©ºã€‚")

    # 2. åŠ è½½æ˜ å°„
    hierarchy_map = load_full_hierarchy_map(MAPPING_FILE)
    if not hierarchy_map:
        print("âŒ ç¼ºå°‘æ˜ å°„æ–‡ä»¶ï¼Œæ— æ³•ç»§ç»­ã€‚")
        return

    # 3. ä¸‹è½½çº¿ä¸Šæ•°æ®
    print("ğŸ“¡ æ­£åœ¨æ‹‰å–ç”³ä¸‡ä¸ªè‚¡å†å²æ•°æ® (ak.stock_industry_clf_hist_sw)...")
    try:
        df_hist = ak.stock_industry_clf_hist_sw()
        print(f"âœ… è·å–æˆåŠŸ! åŸå§‹è®°å½•: {len(df_hist)} æ¡")
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return

    if df_hist is None or df_hist.empty:
        return

    # 4. å¤„ç†ä¸å…¥åº“
    print("âš™ï¸  æ­£åœ¨è¿›è¡Œå±‚çº§æ˜ å°„ä¸å…¥åº“...")
    requests = []
    matched_count = 0

    # è¿™é‡Œçš„ tqdm æ˜¾ç¤ºè¿›åº¦
    for _, row in tqdm(df_hist.iterrows(), total=len(df_hist)):
        symbol = str(row['symbol'])

        # æ—¥æœŸå¤„ç†
        date_raw = row.get('start_date')
        if pd.isna(date_raw) or str(date_raw) == 'NaT':
            continue
        date_str = str(date_raw).split(" ")[0]

        # è¡Œä¸šä»£ç  (è¿™æ˜¯ API è¿”å›çš„)
        raw_code = str(row.get('industry_code', '')).strip()

        # æŸ¥å­—å…¸
        info = hierarchy_map.get(raw_code)

        # æ„é€ åŸºç¡€æ–‡æ¡£
        doc = {
            "symbol": symbol,
            "date": date_str,
            "source": "SHENWAN",
            "industry_code": raw_code, # ä¿ç•™åŸå§‹ä»£ç 
            "updated_at": datetime.now()
        }

        if info:
            # åŒ¹é…æˆåŠŸ: æ³¨å…¥å…¨å±‚çº§ä¿¡æ¯
            doc.update({
                "is_mapped": True,
                # æ ¸å¿ƒå±‚çº§
                "level1_code": info['level1_code'],
                "level1_name": info['level1_name'],
                "level2_code": info['level2_code'],
                "level2_name": info['level2_name'],
                "level3_code": info['level3_code'],
                "level3_name": info['level3_name'],
                # å…¼å®¹æ—§å­—æ®µ (ä¼˜å…ˆæ˜¾ç¤ºæœ€ç»†ç²’åº¦åç§°)
                "industry_name": info['level3_name'] or info['level2_name'] or info['level1_name']
            })
            matched_count += 1
        else:
            # åŒ¹é…å¤±è´¥: å¯èƒ½æ˜¯æ—§ç‰ˆä»£ç  (å¦‚ 440101) ä¸åœ¨ 2021 ç‰ˆ CSV é‡Œ
            doc.update({
                "is_mapped": False,
                "industry_name": f"Unknown_{raw_code}"
            })

        # æ„é€  Upsert è¯·æ±‚ (è™½ç„¶è¡¨å·²ç©ºï¼Œä½†ç”¨ upsert æ›´å®‰å…¨)
        requests.append(UpdateOne(
            {"symbol": symbol, "date": date_str},
            {"$set": doc},
            upsert=True
        ))

        # æ‰¹é‡å†™å…¥
        if len(requests) >= 2000:
            col.bulk_write(requests, ordered=False)
            requests = []

    # å‰©ä½™å†™å…¥
    if requests:
        col.bulk_write(requests, ordered=False)

    # 5. æ€»ç»“
    print("\n" + "="*40)
    print(f"ğŸ‰ é‡æ„å®Œæˆ!")
    print(f"   - æ•°æ®åº“è®°å½•æ•°: {col.count_documents({})}")
    print(f"   - æˆåŠŸæ˜ å°„å±‚çº§: {matched_count} ({(matched_count/len(df_hist)):.1%})")

    if matched_count < len(df_hist) * 0.5:
        print("âš ï¸ è­¦å‘Š: åŒ¹é…ç‡è¾ƒä½ã€‚è¿™é€šå¸¸æ˜¯å› ä¸ºçº¿ä¸Šå†å²æ•°æ®åŒ…å«å¤§é‡ 2014 ç‰ˆæ—§ä»£ç  (4xxxx)ï¼Œ")
        print("   è€Œä½ çš„ CSV ä»…åŒ…å« 2021 ç‰ˆæ–°ä»£ç  (11xxxx/801xxx)ã€‚")
        print("   å»ºè®®: å¯¹äºæœªæ˜ å°„çš„è®°å½•ï¼Œå›æµ‹æ—¶å¯èƒ½æ— æ³•è·å–å…¶æ¿å—å½’å±ã€‚")

    # æŠ½æ ·
    print("\nğŸ” [æŠ½æ ·æ£€æŸ¥] 000001:")
    cursor = col.find({"symbol": "000001"}).sort("date", -1).limit(3)
    for d in cursor:
        print(f"   {d['date']}: {d.get('industry_name')} (Mapped: {d.get('is_mapped')})")

if __name__ == "__main__":
    run()